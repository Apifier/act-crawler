{
	"title": "Cheerio Scraper Input",
	"type": "object",
	"description": "Cheerio Scraper loads <b>Start URLs</b> server-side using Node.js. It executes <b>Page function</b> on each page to extract data from it. To follow links and scrape additional pages, check <b>Use request queue</b> and set <b>Link selector</b> with <b>Pseudo-URLs</b> to specify which links to follow. Alternatively, manually enqueue new links in the <b>Page function</b>. For details, see the actor's <a href='https://apify.com/apify/cheerio-scraper' target='_blank' rel='noopener'>README</a> or the <a href='https://apify.com/docs/scraping/tutorial/introduction' target='_blank' rel='noopener'>Web scraping tutorial</a> in documentation.",
	"schemaVersion": 1,
	"properties": {
		"startUrls": {
			"sectionCaption": "Basic configuration",
			"title": "Start URLs",
			"type": "array",
			"description": "A static list of URLs to scrape. To be able to add new URLs on the fly, enable the <b>Use request queue</b> option.<br><br>For details, see the <a href='https://apify.com/apify/cheerio-scraper#start-urls' target='_blank' rel='noopener'>Start URLs</a>section in README.",
			"prefill": [ { "url": "https://apify.com" } ],
			"editor": "requestListSources"
		},
		"useRequestQueue": {
			"title": "Use request queue",
			"type": "boolean",
			"description": "If enabled, the scraper will support adding new URLs to scrape on the fly, either using the <b>Link selector</b> and <b>Pseudo-URLs</b> options or by calling <code>context.enqueueRequest()</code> inside <b>Page function</b>. Use of the request queue has some overheads, so only enable this option if you need to add URLs dynamically.",
			"default": true,
			"groupCaption": "Options"
		},
		"pseudoUrls": {
			"title": "Pseudo-URLs",
			"type": "array",
			"description": "Specifies what kind of URLs found by the <b>Link selector</b> should be added to the request queue. A pseudo-URL is a URL with regular expressions enclosed in <code>[]</code> brackets, e.g. <code>http://www.example.com/[.*]</code>. This setting only applies if <b>Use request queue</b> is enabled.<br><br>If <b>Pseudo-URLs</b> are omitted, the actor enqueues all links matched by the <b>Link selector</b>.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#pseudo-urls' target='_blank' rel='noopener'>Pseudo-URLs</a> in README.",
			"editor": "pseudoUrls",
			"default": [],
			"prefill": [
				{
					"purl": "https://apify.com[(/[\\w-]+)?]"
				}
			]
		},
		"linkSelector": {
			"title": "Link selector",
			"type": "string",
			"description": "A CSS selector saying which links on the page (<code>&lt;a&gt;</code> elements with <code>href</code> attribute) shall be followed and added to the request queue. This setting only applies if <b>Use request queue</b> is enabled. To filter the links added to the queue, use the <b>Pseudo-URLs</b> field.<br><br>If the <b>Link selector</b> is empty, the page links are ignored.<br><br>For details, see the <a href='https://apify.com/apify/web-scraper#link-selector' target='_blank' rel='noopener'>Link selector</a> README.",
			"editor": "textfield",
			"prefill": "a[href]"
		},
		"keepUrlFragments": {
			"title": "URL #fragments identify unique pages",
			"type": "boolean",
			"description": "Indicates that URL fragments (e.g. <code>http://example.com<b>#fragment</b></code>) should be included when checking whether a URL has already been visited or not. Typically, URL fragments are used for page navigation only and therefore they should be ignored, as they don't identify separate pages. However, some single-page websites use URL fragments to display different pages; in such a case, this option should be enabled.",
			"default": false
		},
		"pageFunction": {
			"title": "Page function",
			"type": "string",
			"description": "JavaScript (ES6) function that is executed in the context of every page loaded server-side. Use it to scrape data from the page, perform actions or add new URLs to the request queue.<br><br>For details, see the <a href='https://apify.com/apify/cheerio-scraper#page-function' target='_blank' rel='noopener'>Page function</a> in README.",
			"prefill": "// The function accepts a single argument: the \"context\" object.\n    // For a complete list of its properties and functions,\n   // see https://apify.com/apify/cheerio-scraper#page-function\n    async function pageFunction(context) {\n    //Pass a destructuring assignment of the necessary properties of the \"context\" object\n    const { $, request, log } = context;\n   const title = $('title').text();\n   //Print URL and title information to the context log\n    log.info(`URL: ${request.url} TITLE: ${title}`);\n    // Return an object with the data extracted from the page.\n    // It will be stored to the resulting dataset.\n    return {\n    url: request.url,\n    title\n    \n};\n    }",
			"editor": "javascript"
		},
		"proxyConfiguration": {
			"title": "Proxy configuration",
			"type": "object",
			"description": "Specifies proxy servers that will be used by the scraper in order to hide its origin.<br><br>For details, see <a href='https://apify.com/apify/cheerio-scraper#proxy-configuration' target='_blank' rel='noopener'>Proxy configuration</a> in README.",
			"prefill": { "useApifyProxy": false },
			"default": {},
			"editor": "proxy"
		},
		"prepareRequestFunction": {
			"sectionCaption": "Advanced configuration",
			"title": "Prepare request function",
			"type": "string",
			"description": "This async function is executed before making a request to a given URL. It's sole argument is an object with two properties: <code>{ request, Apify }</code>. It can be useful to do pre-processing, updating of headers or just setting cookies. IMPORTANT: The return value of this function is irrelevant, you should modify the request instance in place.",
			"prefill": "async function preGotoFunction({ request, Apify }) {\n    /* add your logic here, if needed */\n}",
			"editor": "javascript"
		},
		"maxRequestRetries": {
			"title": "Max request retries",
			"type": "integer",
			"description": "Maximum number of times the request for the page will be retried in case of an error. Setting it to 0 means that the request will be attempted once and will not be retried if it fails.",
			"minimum": 0,
			"default": 3,
			"unit": "retries"
		},
		"maxPagesPerCrawl": {
			"title": "Max pages per run",
			"type": "integer",
			"description": "Maximum number of pages that the scraper will open. 0 means unlimited.",
			"minimum": 0,
			"default": 0,
			"unit": "pages"
		},
		"maxResultsPerCrawl": {
			"title": "Max result records",
			"type": "integer",
			"description": "Maximum number of results that will be saved to dataset. The scraper will terminate afterwards. 0 means unlimited.",
			"minimum": 0,
			"default": 0,
			"unit": "results"
		},
		"maxCrawlingDepth": {
			"title": "Max crawling depth",
			"type": "integer",
			"description": "Defines how many links away from the StartURLs will the scraper descend. 0 means unlimited.",
			"minimum": 0,
			"default": 0
		},
		"maxConcurrency": {
			"title": "Max concurrency",
			"type": "integer",
			"description": "Defines how many pages can be processed by the scraper in parallel. The scraper automatically increases and decreases concurrency based on available system resources. Use this option to set a hard limit.",
			"minimum": 1,
			"default": 50
		},
		"pageLoadTimeoutSecs": {
			"title": "Page load timeout",
			"type": "integer",
			"description": "Maximum time the scraper will allow a web page to load in seconds.",
			"minimum": 1,
			"default": 60,
			"maximum": 360,
			"unit": "secs"
		},
		"pageFunctionTimeoutSecs": {
			"title": "Page function timeout",
			"type": "integer",
			"description": "Maximum time the scraper will wait for the page function to execute in seconds.",
			"minimum": 1,
			"default": 60,
			"maximum": 360,
			"unit": "secs"
		},
		"additionalMimeTypes": {
			"title": "Additional mime types",
			"type": "array",
			"description": "By default text/html and application/xhtml+xml are supported. Note that while the default Accept header will allow any Content-Type to be received, HTML and XML are preferred over JSON and other types. Thus, if you're allowing additional mime types and you're still receiving invalid responses, be sure to override the Accept header either in Start URL or in the Prepare request function.",
			"editor": "json",
			"default": [],
			"prefill": []
		},
		"debugLog": {
			"title": "Debug log",
			"type": "boolean",
			"description": "Debug messages will be included in the log. Use <code>context.log.debug('message')</code> to log your own debug messages.",
			"default": false,
			"groupCaption": "Options",
			"groupDescription": "Scraper settings"
		},
		"ignoreSslErrors": {
			"title": "Ignore SSL errors",
			"type": "boolean",
			"description": "Scraper will ignore SSL certificate errors.",
			"default": false
		},
		"useCookieJar": {
			"title": "(UNSTABLE) Save cookies",
			"type": "boolean",
			"description": "The scraper will use a cookie jar to persist cookies between requests. This is a temporary solution and the feature is UNSTABLE, meaning that it will most likely be removed in the future and replaced with a different API. Use at your own risk.",
			"default": false
		},
		"initialCookies": {
			"title": "Initial cookies",
			"type": "array",
			"description": "The provided cookies will be pre-set to all pages the scraper opens.",
			"default": [],
			"prefill": [],
			"editor": "json"
		},
		"customData": {
			"title": "Custom data",
			"type": "object",
			"description": "This object will be available on pageFunction's context as customData.",
			"default": {},
			"prefill": {},
			"editor": "json"
		}
	},
	"required": [ "startUrls", "pageFunction" ]
}
