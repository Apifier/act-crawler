{
    "title": "Web Scraper Input",
    "type": "object",
    "description": "Use the following form to configure Web Scraper. The Start URLs and Page function are required and all other fields are optional. If you want to use Pseudo URLs, you also need to Use request queue. To learn more about the different options, click on their title or on the nearby question marks. For details about the Page function visit the actor's README in the ACTOR INFO tab. We also have a <a href='https://apify.com/docs/scraping/tutorial/introduction' target='_blank' rel='noopener'>step by step tutorial</a> to help you with the basics.\n\nIf you get stuck and it seems that you can no longer progress with the Web Scraper, try <a href='https://apify.com/docs/scraping/tutorial/puppeteer-scraper' target='_blank' rel='noopener'>Puppeteer Scraper</a>. It's a more powerful tool.",
    "schemaVersion": 1,
    "properties": {
        "startUrls": {
            "sectionCaption": "Basic configuration",
            "title": "Start URLs",
            "type": "array",
            "description": "A static list of URLs to scrape. In order to be able to add new URLs on the fly, enable the <b>Use request queue</b> option.",
            "prefill": [
                { "url": "https://apify.com" }
            ],
            "editor": "requestListSources"
        },
        "useRequestQueue": {
            "title": "Use request queue",
            "type": "boolean",
            "description": "If enabled, the scraper will enable adding new URLs to scrape on the fly, either using the <b>Link selector</b> and <b>Pseudo-URLs</b> options or by calling <code>context.enqueueRequest()</code> from the <b>Page function</b>. Usage of the request queue has some overheads, so only enable this option if you need to add URLs dynamically.",
            "default": true,
            "groupCaption": "Options"
        },
        "keepUrlFragments": {
            "title": "Keep URL fragments",
            "type": "boolean",
            "description": "Indicates that URL fragments (i.e. <code>http://example.com<b>#fragment</b></code>) should be considered when checking whether a page has already been crawled or not. Typically, URL fragments are used as internal page anchors and therefore they should be ignored, because they don't represent separate pages. However, some single-page websites might use URL fragments to represent page parameters; in such a case, this option should be enabled.",
            "default": false
        },
        "linkSelector": {
            "title": "Link selector",
            "type": "string",
            "description": "A CSS selector for <code>&lt;a&gt;</code> elements on the page whose links (the <code>href</code> attribute) are to be automatically added to the crawling queue. This setting only applies if the <b>Use request queue</b> option is enabled. To filter the links added to the queue, use the <b>Pseudo-URLs</b> setting.<br><br>If the <b>Link selector</b> is empty, the page links are ignored.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#link-selector' target='_blank' rel='noopener'>Link selector</a> in README.",
            "editor": "textfield",
            "prefill": "a[href]"
        },
        "pseudoUrls": {
            "title": "Pseudo-URLs",
            "type": "array",
            "description": "Filters which links found by the <b>Link selector</b> should be added to the request queue. A pseudo-URL is a URL with regular expressions enclosed in <code>[]</code> brackets, e.g. <code>http://www.example.com/[.*]</code>. This setting only applies if the <b>Use request queue</b> option is enabled.<br><br>If <b>Pseudo-URLs</b> are omitted, the actor enqueues all links matched by the <b>Link selector</b>.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#pseudo-urls' target='_blank' rel='noopener'>Pseudo-URLs</a> in README.",
            "editor": "pseudoUrls",
            "default": [],
            "prefill": [
                {
                    "purl": "https://apify.com[(/[\\w-]+)?]"
                }
            ]
        },
        "pageFunction": {
            "title": "Page function",
            "type": "string",
            "description": "JavaScript (ES6) function that is executed on every web page loaded. Use it to scrape data from the page, perform actions or add new URLs to the request queue.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#page-function' target='_blank' rel='noopener'>Page function</a> in README.",
            "prefill": "// The page function accepts a single argument: the \"context\" object.\n// For a complete list of its properties and functions,\n// see https://apify.com/apify/web-scraper#page-function \nasync function pageFunction(context) {\n    // jQuery library is handy for finding DOM elements and extracing data from them.\n    // To use it, make sure the \"Injected libraries -> jQuery\" option.\n    const $ = context.jQuery;\n    const pageTitle = $('title').text();\n\n    // The log.info() function \n    context.log.info(`URL: ${context.request.url} TITLE: ${title}`);\n\n    // The returned object is stored to dataset with scraper results\n    // To save data just return an object with the requested properties.\n    return {\n        url: context.request.url,\n        pageTitle\n    };\n}",
            "editor": "javascript"
        },
        "injectJQuery": {
            "title": "jQuery",
            "type": "boolean",
            "description": "If enabled, the <a href='http://jquery.com' target='_blank' rel='noopener'>jQuery</a> library will be injected into a web page before <b>Page function</b> is invoked. Note that the jQuery object will not be registered into global namespace in order to avoid conflicts with libraries used by the web page. It can only be accessed through <code>context.jQuery</code> in <code>Page function</code>.",
            "default": true,
            "groupCaption": "Injected libraries"
        },
        "injectUnderscore": {
            "title": "Underscore",
            "type": "boolean",
            "description": "If enabled, the <a href='http://underscorejs.org' target='_blank' rel='noopener'>Underscore.js</a> library will be injected into a web page before <b>Page function</b> is invoked. Note that the Underscore object will not be registered into global namespace in order to avoid conflicts with libraries used by the web page. It can only be accessed through <code>context.underscoreJs</code> in <code>Page function</code>.",
            "default": false
        },
        "proxyConfiguration": {
            "sectionCaption": "Proxy and browser configuration",
            "title": "Proxy configuration",
            "type": "object",
            "description": "Specifies proxy servers that will be used by the scraper in order to hide its origin.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#proxy-configuration' target='_blank' rel='noopener'>Link selector</a> in README.",
            "prefill": { "useApifyProxy": false },
            "default": {},
            "editor": "proxy"
        },
        "initialCookies": {
            "title": "Initial cookies",
            "type": "array",
            "description": "A JSON array with cookies that will be set to every Chrome browser tab opened, in the format accepted by Puppeteer's <a href='https://pptr.dev/#?product=Puppeteer&show=api-pagesetcookiecookies' target='_blank' rel='noopener'><code>Page.setCookie()</code></a> function. This option is useful for transferring a logged-in session from an external web browser. For details how to do this, read this <a href='https://help.apify.com/en/articles/1444249-log-in-to-website-by-transferring-cookies-from-web-browser-legacy' target='_blank' rel='noopener'>help article</a>.",
            "default": [],
            "prefill": [],
            "editor": "json"
        },
        "useChrome": {
            "title": "Use Chrome",
            "type": "boolean",
            "description": "If enabled, the scraper will use a real Chrome browser instead of Chromium bundled with Puppeteer. This option may help bypass certain anti-scraping protections, but might make the scraper unstable. Use at your own risk \uD83D\uDE42",
            "default": false,
            "groupCaption": "Browser masking"
        },
        "useStealth": {
            "title": "Use stealth mode",
            "type": "boolean",
            "description": "If enabled, the scraper will apply various browser emulation techniques to match a real user as closely as possible, in order to bypass around certain anti-scraping protections. This feature works best in conjunction with the <b>Use Chrome</b> option, but it also carries a risk of making the scraper unstable.",
            "default": false
        },
        "ignoreSslErrors": {
            "title": "Ignore SSL errors",
            "type": "boolean",
            "description": "If enabled, the scraper will ignore SSL/TLS certificate errors. Use at your own risk.",
            "default": false,
            "groupCaption": "Security"
        },
        "ignoreCorsAndCsp": {
            "title": "Ignore CORS and CSP",
            "type": "boolean",
            "description": "If enabled, the scraper will ignore Content Security Policy (CSP) and Cross-Origin Resource Sharing (CORS) settings of visited pages and requested domains. This enables you to freely use XHR/Fetch to make HTTP requests from the <b>Page function</b>.",
            "default": false
        },
        "downloadMedia": {
            "sectionCaption": "Performance and limits",
            "title": "Download media files",
            "type": "boolean",
            "description": "If enabled, the scraper will download media such as images, fonts, videos and sound files as usual. Disabling this option might speed up the scrape, but certain websites could stop working correctly.",
            "default": true,
            "groupCaption": "Page resources",
            "groupDescription": "Settings that help to disable downloading web page resources."
        },
        "downloadCss": {
            "title": "Download CSS files",
            "type": "boolean",
            "description": "If enabled, the scraper will download CSS files with stylesheets. Disabling this option may speed up the scrape, but certain websites could stop working correctly.",
            "default": true
        },
        "maxRequestRetries": {
            "title": "Max page retries",
            "type": "integer",
            "description": "The maximum number of times the scraper will retry to open a web page on error (both page load error or exception from <b>Page function</b>). If set to <code>0</code>, the page will be considered failed after first error.",
            "minimum": 0,
            "default": 3
        },
        "maxPagesPerCrawl": {
            "title": "Max pages per run",
            "type": "integer",
            "description": "The maximum number of pages that the scraper will load. The scraper will stop when this limit is reached. It's always a good idea to set this limit in order to prevent infinite loops in misconfigured scrapers. Note that the actual number of pages loaded might be slightly higher than this value.<br><br>If set to <code>0</code>, there is no limit.",
            "minimum": 0,
            "default": 0,
            "unit": "pages"
        },
        "maxResultsPerCrawl": {
            "title": "Max result records",
            "type": "integer",
            "description": "The maximum number of records that will be saved to the resulting dataset. The scraper will stop when this limit is reached. <br><br>If set to <code>0</code>, there is no limit.",
            "minimum": 0,
            "default": 0,
            "unit": "results"
        },
        "maxCrawlingDepth": {
            "title": "Max crawling depth",
            "type": "integer",
            "description": "Specifies how many links away from the <b>Start URLs</b> the scraper will descend. This value is a safeguard against infinite crawling depths for misconfigured scrapers. Note that pages added using <code>enqueuePage()</code> in <b>Page function</b> are not subject to the maximum depth constraint. <br><br>If set to <code>0</code>, there is no limit.",
            "minimum": 0,
            "default": 0
        },
        "maxConcurrency": {
            "title": "Max concurrency",
            "type": "integer",
            "description": "Defines how many pages can be processed by the scraper in parallel. The scraper automatically increases and decreases concurrency based on available system resources. This option enables you to set an upper limit, for example to reduce the load on the target website.",
            "minimum": 1,
            "default": 50
        },
        "pageLoadTimeoutSecs": {
            "title": "Page load timeout",
            "type": "integer",
            "description": "The maximum amount of time the scraper will wait for a web page to load, in seconds. If the web page does not load in this timeframe, it is considered to have failed and will be retried, similarly as with other page load errors.",
            "minimum": 1,
            "default": 60,
            "maximum": 360,
            "unit": "seconds"
        },
        "pageFunctionTimeoutSecs": {
            "title": "Page function timeout",
            "type": "integer",
            "description": "The maximum amount of time the scraper will wait for <b>Page function</b> to execute, in seconds.",
            "minimum": 1,
            "default": 60,
            "maximum": 360,
            "unit": "seconds"
        },
        "waitUntil": {
            "title": "Navigation waits until",
            "type": "array",
            "description": "Contains a JSON array with names of page events to wait for when loading each web page. The scraper will wait until <b>all</b> of the events are triggered in the web page before executing <b>Page function</b>. Available events are <code>domcontentloaded</code>, <code>load</code>, <code>networkidle2</code> and <code>networkidle0</code>.<br><br>For details, see <a href='https://pptr.dev/#?product=Puppeteer&show=api-pagegotourl-options' target='_blank' rel='noopener'><code>waitUntil</code> option</a> in Puppeteer's <code>Page.goto()</code> function documentation.",
            "default": ["networkidle2"],
            "prefill": ["networkidle2"],
            "editor": "json"
        },
        "debugLog": {
            "sectionCaption": "Advanced configuration",
            "title": "Debug log",
            "type": "boolean",
            "description": "If enabled, the actor log will include debug messages. Beware that this can be quite verbose. Use <code>context.log.debug('message')</code> to log your own debug messages in <b>Page function</b>.",
            "default": false,
            "groupCaption": "Enable logs",
            "groupDescription": "Logs settings"
        },
        "browserLog": {
            "title": "Browser log",
            "type": "boolean",
            "description": "If enabled, the actor log will include console messages produced by JavaScript executed by the web pages (e.g. using <code>console.log()</code>). Beware that this may result in the log being flooded by error messages, warnings and other messages of little value, especially with high concurrency.",
            "default": false
        },
        "customData": {
            "title": "Custom data",
            "type": "object",
            "description": "A custom JSON object that is passed to <b>Page function</b> as <code>context.customData</code>. This setting is useful when invoking the scraper via API, in order to pass some arbitrary parameters to the user code in <b>Page function</b>.",
            "default": {},
            "prefill": {},
            "editor": "json"
        }
    },
    "required": ["startUrls", "pageFunction"]
}
