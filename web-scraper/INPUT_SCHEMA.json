{
    "title": "Web Scraper Input",
    "type": "object",
    "description": "Use the following form to configure Web Scraper. The Start URLs and Page function are required and all other fields are optional. If you want to use Pseudo URLs, you also need to Use request queue. To learn more about the different options, click on their title or on the nearby question marks. For details about the Page function visit the actor's README in the ACTOR INFO tab. We also have a <a href='https://apify.com/docs/scraping/tutorial/introduction' target='_blank' rel='noopener'>step by step tutorial</a> to help you with the basics.\n\nIf you get stuck and it seems that you can no longer progress with the Web Scraper, try <a href='https://apify.com/docs/scraping/tutorial/puppeteer-scraper' target='_blank' rel='noopener'>Puppeteer Scraper</a>. It's a more powerful tool.",
    "schemaVersion": 1,
    "properties": {
        "startUrls": {
            "sectionCaption": "Basic configuration",
            "title": "Start URLs",
            "type": "array",
            "description": "A static list of URLs to crawl. To be able to add new URLs to crawl on the fly, enable the <b>Use request queue</b> option.",
            "prefill": [
                { "url": "https://apify.com" }
            ],
            "editor": "requestListSources"
        },
        "useRequestQueue": {
            "title": "Use request queue",
            "type": "boolean",
            "description": "The request queue lets you add new URLs to crawl on the fly, either using the <b>Link selector</b> and <b>Pseudo-URLs</b> options or by calling <code>context.enqueueRequest()</code> from your <b>Page function</b>. Usage of the request queue has some overheads, so only enable this option if you want to add URLs dynamically.",
            "default": true
        },
        "linkSelector": {
            "title": "Link selector",
            "type": "string",
            "description": "A CSS selector for <code>&lt;a&gt;</code> elements on the page, whose links (from the <code>href</code> attribute) are to be automatically added to the crawling queue. This setting only has effect if the <b>Use request queue</b> option is enabled. To filter which links are added to the queue, use the <b>Pseudo-URLs</b> setting.<br><br>If the <b>Link selector</b> is empty, the page links are ignored.",
            "editor": "textfield",
            "prefill": "a[href]"
        },
        "pseudoUrls": {
            "title": "Pseudo-URLs",
            "type": "array",
            "description": "Filter which links matching the <b>Link selector</b> are added to the crawling queue. A pseudo-URL is a URL with regular expressions enclosed in <code>[]</code> brackets, e.g. <code>http://www.example.com/[.*]</code>. This setting only has effect if the <b>Use request queue</b> option is enabled.<br><br>If <b>Pseudo-URLs</b> are omitted, the actor enqueues all links matched by the <b>Link selector</b>.",
            "editor": "pseudoUrls",
            "default": [],
            "prefill": [
                {
                    "purl": "https://apify.com[(/[\\w-]+)?]"
                }
            ]
        },
        "keepUrlFragments": {
            "title": "Keep URL fragments",
            "type": "boolean",
            "description": "Indicates that URL fragment identifiers (i.e. <code>http://example.com/page#<b>this-guy-here</b></code>) should be considered when checking whether a page has already been crawled. Typically, URL fragments are used as internal page anchors and therefore they should be ignored because they don't represent separate pages. However, many AJAX-based website nowadays use URL fragment to represent page parameters; in such cases, this option should be enabled.",
            "default": false
        },
        "pageFunction": {
            "title": "Page function",
            "type": "string",
            "description": "JavaScript function that is executed on every crawled web page. Use it to extract data from the page, perform actions or enqueue new URLs to crawl.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#page-function'>Page function</a> in actor README.",
            "prefill": "async function pageFunction(context) {\n    // See README for context properties. If the syntax is unfamiliar see the link\n    // https://javascript.info/destructuring-assignment#object-destructuring\n    const { request, log, jQuery } = context;\n\n    // To be able to use jQuery as $, one needs save it into a variable\n    // and select the inject jQuery option. We've selected it for you.\n    const $ = jQuery;\n    const title = $('title').text();\n\n    // This is yet another new feature of Javascript called template strings.\n    // https://javascript.info/string#quotes\n    log.info(`URL: ${request.url} TITLE: ${title}`);\n\n    // To save data just return an object with the requested properties.\n    return {\n        url: request.url,\n        title\n    };\n}",
            "editor": "javascript"
        },
        "injectJQuery": {
            "title": "jQuery",
            "type": "boolean",
            "description": "If enabled, the <a href='http://jquery.com' target='_blank' rel='noopener'>jQuery</a> library should be injected into each page before <b>Page function</b> is invoked. Note that the jQuery object will not be registered into global namespace in order to avoid conflicts with libraries used by the web page. It can only be accessed through <code>context.jQuery</code>.",
            "default": true,
            "groupCaption": "Inject library"
        },
        "injectUnderscore": {
            "title": "Underscore",
            "type": "boolean",
            "description": "If enabled, the <a href='http://underscorejs.org' target='_blank' rel='noopener'>Underscore.js</a> library should be injected into each page before <b>Page function</b> is invoked. Note that the Underscore object will not be registered into global namespace in order to avoid conflicts with libraries used by the web page. It can only be accessed through <code>context.underscoreJs</code>.",
            "default": false
        },
        "proxyConfiguration": {
            "sectionCaption": "Proxy and browser configuration",
            "title": "Proxy configuration",
            "type": "object",
            "description": "Specifies proxy servers that will be used by the scraper in order to hide its origin.",
            "prefill": { "useApifyProxy": false },
            "default": {},
            "editor": "proxy"
        },
        "initialCookies": {
            "title": "Initial cookies",
            "type": "array",
            "description": "A JSON array with cookies that will be set to every Chrome browser tab opened, in the format accepted by Puppeteer's <a href='https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md#pagesetcookiecookies' target='_blank' rel='noopener'><code>Page.setCookie()</code></a> function. This option is useful for transferring a logged-in session from an external web browser. For details how to do this, read this <a href='https://help.apify.com/en/articles/1444249-log-in-to-website-by-transferring-cookies-from-web-browser-legacy' target='_blank' rel='noopener'>help article</a>.",
            "default": [],
            "prefill": [],
            "editor": "json"
        },
        "useChrome": {
            "title": "Use Chrome",
            "type": "boolean",
            "description": "If enabled, the scraper will use a real Chrome browser instead of Chromium bundled with Puppeteer. This option may help bypass certain anti-scraping protections, but might make the scraper unstable. Use at your own risk \uD83D\uDE42",
            "default": false,
            "groupCaption": "Browser masking"
        },
        "useStealth": {
            "title": "Use stealth mode",
            "type": "boolean",
            "description": "If enabled, the scraper will apply various browser emulation techniques to match a real user as closely as possible, in order to bypass around certain anti-scraping protections. This feature works best in conjunction with the <b>Use Chrome</b> option, but it also carries a risk of making the scraper unstable.",
            "default": false
        },
        "ignoreSslErrors": {
            "title": "Ignore SSL errors",
            "type": "boolean",
            "description": "Scraper will ignore SSL certificate errors.",
            "default": false,
            "groupCaption": "Security"
        },
        "ignoreCorsAndCsp": {
            "title": "Ignore CORS and CSP",
            "type": "boolean",
            "description": "Scraper will ignore Content Security Policy (CSP) and Cross-Origin Resource Sharing (CORS) settings of visited pages and requested domains. This enables you to freely use XHR/Fetch to make HTTP requests from the scraper.",
            "default": false
        },
        "downloadMedia": {
            "sectionCaption": "Performance and limits",
            "title": "Download media",
            "type": "boolean",
            "description": "Scraper will download media such as images, fonts, videos and sounds. Disabling this may speed up the scrape, but certain websites could stop working correctly.",
            "default": true,
            "groupCaption": "Resources",
            "groupDescription": "Settings that help to disable downloading web page resources."
        },
        "downloadCss": {
            "title": "Download CSS",
            "type": "boolean",
            "description": "Scraper will download CSS stylesheets. Disabling this may speed up the scrape, but certain websites could stop working correctly.",
            "default": true
        },
        "maxRequestRetries": {
            "title": "Max request retries",
            "type": "integer",
            "description": "Maximum number of times the request for the page will be retried in case of an error. Setting it to 0 means that the request will be attempted once and will not be retried if it fails.",
            "minimum": 0,
            "default": 3,
            "unit": "retries"
        },
        "maxPagesPerCrawl": {
            "title": "Max pages per run",
            "type": "integer",
            "description": "Maximum number of pages that the scraper will open. 0 means unlimited.",
            "minimum": 0,
            "default": 0,
            "unit": "pages"
        },
        "maxResultsPerCrawl": {
            "title": "Max result records",
            "type": "integer",
            "description": "Maximum number of results that will be saved to dataset. The scraper will terminate afterwards. 0 means unlimited.",
            "minimum": 0,
            "default": 0,
            "unit": "results"
        },
        "maxCrawlingDepth": {
            "title": "Max crawling depth",
            "type": "integer",
            "description": "Defines how many links away from the StartURLs will the scraper descend. 0 means unlimited.",
            "minimum": 0,
            "default": 0
        },
        "maxConcurrency": {
            "title": "Max concurrency",
            "type": "integer",
            "description": "Defines how many pages can be processed by the scraper in parallel. The scraper automatically increases and decreases concurrency based on available system resources. Use this option to set a hard limit.",
            "minimum": 1,
            "default": 50
        },
        "pageLoadTimeoutSecs": {
            "title": "Page load timeout",
            "type": "integer",
            "description": "Maximum time the scraper will allow a web page to load, in seconds.",
            "minimum": 1,
            "default": 60,
            "maximum": 360,
            "unit": "seconds"
        },
        "pageFunctionTimeoutSecs": {
            "title": "Page function timeout",
            "type": "integer",
            "description": "Maximum time the scraper will wait for the page function to execute, in seconds.",
            "minimum": 1,
            "default": 60,
            "maximum": 360,
            "unit": "seconds"
        },
        "waitUntil": {
            "title": "Navigation wait until",
            "type": "array",
            "description": "The scraper will wait until the selected events are triggered in the page before executing the page function. Available events are <code>domcontentloaded</code>, <code>load</code>, <code>networkidle2</code> and <code>networkidle0</code>. <a href='https://pptr.dev/#?product=Puppeteer&show=api-pagegotourl-options' target='_blank'>See Puppeteer docs</a>.",
            "default": ["networkidle2"],
            "prefill": ["networkidle2"],
            "editor": "json"
        },
        "debugLog": {
            "sectionCaption": "Advanced configuration",
            "title": "Debug log",
            "type": "boolean",
            "description": "Debug messages will be included in the log. Use <code>context.log.debug('message')</code> to log your own debug messages.",
            "default": false,
            "groupCaption": "Enable logs",
            "groupDescription": "Logs settings"
        },
        "browserLog": {
            "title": "Browser log",
            "type": "boolean",
            "description": "Console messages from the Browser will be included in the log. This may result in the log being flooded by error messages, warnings and other messages of little value, especially with high concurrency.",
            "default": false
        },
        "customData": {
            "title": "Custom data",
            "type": "object",
            "description": "This object will be available on pageFunction's context as customData.",
            "default": {},
            "prefill": {},
            "editor": "json"
        }
    },
    "required": ["startUrls", "pageFunction"]
}
